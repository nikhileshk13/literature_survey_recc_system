{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('allenai-specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data and emeddings for first 10 chunks\n",
    "embeddings1 = np.load(rf'N:\\arxiv_dataset\\github_121420\\chunks_1000\\embeddings\\0.npy')\n",
    "data1 = pd.read_csv(rf'N:\\arxiv_dataset\\github_121420\\chunks_1000\\data\\0.csv')\n",
    "for i in range(1, 10):\n",
    "    embeddings = np.load(rf'N:\\arxiv_dataset\\github_121420\\chunks_1000\\embeddings\\{i}.npy')\n",
    "    data = pd.read_csv(rf'N:\\arxiv_dataset\\github_121420\\chunks_1000\\data\\{i}.csv')\n",
    "    embeddings1 = np.concatenate((embeddings1, embeddings), axis=0)\n",
    "    data1 = pd.concat([data1, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to preprocess and generate embeddings for user input\n",
    "def preprocess_input(user_input, model):\n",
    "\n",
    "    # cleaning the user input if needed\n",
    "    user_input.replace('\\n', ' ')\n",
    "    latex_converter = LatexNodes2Text()\n",
    "    user_input = latex_converter.latex_to_text(user_input)\n",
    "\n",
    "    # generating embeddings for the user input\n",
    "    user_embeddings = model.encode(user_input)\n",
    "\n",
    "    return user_embeddings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_user_input = '''Kiosk machines have gained good popularity among\n",
    "the general public as they are easy to operate and provide a good\n",
    "interactive interface. As a result, multiple users use the kiosk\n",
    "machine throughout the day to find the information they are\n",
    "looking for. Users interact with the kiosk machine by the means of\n",
    "touching its screen or using the buttons. Due to this, it is observed\n",
    "that throughout the day hundreds or even thousands of people\n",
    "end up touching the surface of the kiosk machine. Because of this\n",
    "hygiene cannot be maintained as it is not possible to sanitize the\n",
    "kiosk machine after each use. This has become a serious issue\n",
    "considering the effects that the Covid-19 pandemic had on the\n",
    "world. Multiple people touching the same surface is one of the\n",
    "most common ways through which the virus can spread. To help\n",
    "deal with this problem we have designed a gesture control system\n",
    "using deep learning techniques through which kiosk machines can\n",
    "be operated in a touch-less way.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = preprocess_input(sample_user_input, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to find consine similarity\n",
    "def cosine_sim(user_embeddings, data_embeddings):\n",
    "    similarity = cosine_similarity([user_embeddings], data_embeddings)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_sim(user_embeddings, embeddings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['score'] = similarity[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>submitter</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>update_date</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2248</td>\n",
       "      <td>Jegor Uglov Mr</td>\n",
       "      <td>Comparing Robustness of Pairwise and Multiclass Neural-Network Systems   for Face Recognition</td>\n",
       "      <td>10.1155/2008/468693</td>\n",
       "      <td>Noise, corruptions and variations in face images can seriously hurt the performance of face recognition systems. To make such systems robust, multiclass neuralnetwork classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise.</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>0.700318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>2705</td>\n",
       "      <td>Nathalie Villa</td>\n",
       "      <td>Un résultat de consistance pour des SVM fonctionnels par interpolation   spline</td>\n",
       "      <td>10.1016/j.crma.2006.09.025</td>\n",
       "      <td>This Note proposes a new methodology for function classification with Support Vector Machine (SVM). Rather than relying on projection on a truncated Hilbert basis as in our previous work, we use an implicit spline interpolation that allows us to compute SVM on the derivatives of the studied functions. To that end, we propose a kernel defined directly on the discretizations of the observed functions. We show that this method is universally consistent.</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>0.699330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>5740</td>\n",
       "      <td>Sophie Frisch</td>\n",
       "      <td>Parametrization of Pythagorean triples by a single triple of polynomials</td>\n",
       "      <td>10.1016/j.jpaa.2007.05.019</td>\n",
       "      <td>It is well known that Pythagorean triples can be parametrized by two triples of polynomials with integer coefficients. We show that no single triple of polynomials with integer coefficients in any number of variables is sufficient, but that there exists a parametrization of Pythagorean triples by a single triple of integer-valued polynomials.</td>\n",
       "      <td>2011-06-29</td>\n",
       "      <td>0.674661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>2704</td>\n",
       "      <td>Nathalie Villa</td>\n",
       "      <td>Support vector machine for functional data classification</td>\n",
       "      <td>10.1016/j.neucom.2005.12.010</td>\n",
       "      <td>In many applications, input data are sampled functions taking their values in infinite dimensional spaces rather than standard vectors. This fact has complex consequences on data analysis algorithms that motivate modifications of them. In fact most of the traditional data analysis tools for regression, classification and clustering have been adapted to functional inputs under the general name of functional Data Analysis (FDA). In this paper, we investigate the use of Support Vector Machines (SVMs) for functional data analysis and we focus on the problem of curves discrimination. SVMs are large margin classifier tools based on implicit non linear mappings of the considered data into high dimensional spaces thanks to kernels. We show how to define simple kernels that take into account the unctional nature of the data and lead to consistent classification. Experiments conducted on real world data emphasize the benefit of taking into account some functional aspects of the problems.</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>0.668528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>9632</td>\n",
       "      <td>Yanxia Zhang</td>\n",
       "      <td>Two novel approaches for photometric redshift estimation based on SDSS   and 2MASS databases</td>\n",
       "      <td>10.1088/1009-9271/8/1/13</td>\n",
       "      <td>We investigate two training-set methods: support vector machines (SVMs) and Kernel Regression (KR) for photometric redshift estimation with the data from the Sloan Digital Sky Survey Data Release 5 and Two Micron All Sky Survey databases. We probe the performances of SVMs and KR for different input patterns. Our experiments show that the more parameters considered, the accuracy doesn't always increase, and only when appropriate parameters chosen, the accuracy can improve. Moreover for different approaches, the best input pattern is different. With different parameters as input, the optimal bandwidth is dissimilar for KR. The rms errors of photometric redshifts based on SVM and KR methods are less than 0.03 and 0.02, respectively. Finally the strengths and weaknesses of the two approaches are summarized. Compared to other methods of estimating photometric redshifts, they show their superiorities, especially KR, in terms of accuracy.</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>0.656348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5128</td>\n",
       "      <td>Kenichiro Aoki</td>\n",
       "      <td>A small tabletop experiment for a direct measurement of the speed of   light</td>\n",
       "      <td>10.1119/1.2919743</td>\n",
       "      <td>A small tabletop experiment for a direct measurement of the speed of light to an accuracy of few percent is described. The experiment is accessible to a wide spectrum of undergraduate students, in particular to students not majoring in science or engineering. The experiment may further include a measurement of the index of refraction of a sample. Details of the setup and equipment are given. Results and limitations of the experiment are analyzed, partly based on our experience in employing the experiment in our student laboratories. Safety considerations are also discussed.</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>0.652193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>2377</td>\n",
       "      <td>Jo\\~ao Bastos</td>\n",
       "      <td>A multivariate approach to heavy flavour tagging with cascade training</td>\n",
       "      <td>10.1088/1748-0221/2/11/P11007</td>\n",
       "      <td>This paper compares the performance of artificial neural networks and boosted decision trees, with and without cascade training, for tagging b-jets in a collider experiment. It is shown, using a Monte Carlo simulation of WH → lν qq̅ events, that for a b-tagging efficiency of 50</td>\n",
       "      <td>2011-01-27</td>\n",
       "      <td>0.649132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>9073</td>\n",
       "      <td>Riccardo Zecchina</td>\n",
       "      <td>Efficient supervised learning in networks with binary synapses</td>\n",
       "      <td>10.1073/pnas.0700324104</td>\n",
       "      <td>Recent experimental studies indicate that synaptic changes induced by neuronal activity are discrete jumps between a small number of stable states. Learning in systems with discrete synapses is known to be a computationally hard problem. Here, we study a neurobiologically plausible on-line learning algorithm that derives from Belief Propagation algorithms. We show that it performs remarkably well in a model neuron with binary synapses, and a finite number of `hidden' states per synapse, that has to learn a random classification task. Such system is able to learn a number of associations close to the theoretical limit, in time which is sublinear in system size. This is to our knowledge the first on-line algorithm that is able to achieve efficiently a finite number of patterns learned per binary synapse. Furthermore, we show that performance is optimal for a finite number of hidden states which becomes very small for sparse coding. The algorithm is similar to the standard `perceptron' learning algorithm, with an additional rule for synaptic transitions which occur only if a currently presented pattern is `barely correct'. In this case, the synaptic changes are meta-plastic only (change in hidden states and not in actual synaptic state), stabilizing the synapse in its current state. Finally, we show that a system with two visible states and K hidden states is much more robust to noise than a system with K visible states. We suggest this rule is sufficiently simple to be easily implemented by neurobiological systems or in hardware.</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>0.648897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4114</td>\n",
       "      <td>Niels Ubbelohde</td>\n",
       "      <td>Bimodal Counting Statistics in Single Electron Tunneling through a   Quantum Dot</td>\n",
       "      <td>10.1103/PhysRevB.76.155307</td>\n",
       "      <td>We explore the full counting statistics of single electron tunneling through a quantum dot using a quantum point contact as non-invasive high bandwidth charge detector. The distribution of counted tunneling events is measured as a function of gate and source-drain-voltage for several consecutive electron numbers on the quantum dot. For certain configurations we observe super-Poissonian statistics for bias voltages at which excited states become accessible. The associated counting distributions interestingly show a bimodal characteristic. Analyzing the time dependence of the number of electron counts we relate this to a slow switching between different electron configurations on the quantum dot.</td>\n",
       "      <td>2009-11-13</td>\n",
       "      <td>0.646445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>8489</td>\n",
       "      <td>Marian Anghel</td>\n",
       "      <td>Consistency of support vector machines for forecasting the evolution of   an unknown ergodic dynamical system from observations with unknown noise</td>\n",
       "      <td>10.1214/07-AOS562</td>\n",
       "      <td>We consider the problem of forecasting the next (observable) state of an unknown ergodic dynamical system from a noisy observation of the present state. Our main result shows, for example, that support vector machines (SVMs) using Gaussian RBF kernels can learn the best forecaster from a sequence of noisy observations if (a) the unknown observational noise process is bounded and has a summable α-mixing rate and (b) the unknown ergodic dynamical system is defined by a Lipschitz continuous function on some compact subset of ℝ^d and has a summable decay of correlations for Lipschitz continuous functions. In order to prove this result we first establish a general consistency result for SVMs and all stochastic processes that satisfy a mixing notion that is substantially weaker than α-mixing.</td>\n",
       "      <td>2009-04-07</td>\n",
       "      <td>0.642360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          submitter  \\\n",
       "256  2248     Jegor Uglov Mr   \n",
       "713  2705     Nathalie Villa   \n",
       "760  5740      Sophie Frisch   \n",
       "712  2704     Nathalie Villa   \n",
       "668  9632       Yanxia Zhang   \n",
       "148  5128     Kenichiro Aoki   \n",
       "385  2377      Jo\\~ao Bastos   \n",
       "109  9073  Riccardo Zecchina   \n",
       "130  4114    Niels Ubbelohde   \n",
       "521  8489      Marian Anghel   \n",
       "\n",
       "                                                                                                                                                  title  \\\n",
       "256                                                       Comparing Robustness of Pairwise and Multiclass Neural-Network Systems   for Face Recognition   \n",
       "713                                                                     Un résultat de consistance pour des SVM fonctionnels par interpolation   spline   \n",
       "760                                                                            Parametrization of Pythagorean triples by a single triple of polynomials   \n",
       "712                                                                                           Support vector machine for functional data classification   \n",
       "668                                                        Two novel approaches for photometric redshift estimation based on SDSS   and 2MASS databases   \n",
       "148                                                                        A small tabletop experiment for a direct measurement of the speed of   light   \n",
       "385                                                                              A multivariate approach to heavy flavour tagging with cascade training   \n",
       "109                                                                                      Efficient supervised learning in networks with binary synapses   \n",
       "130                                                                    Bimodal Counting Statistics in Single Electron Tunneling through a   Quantum Dot   \n",
       "521  Consistency of support vector machines for forecasting the evolution of   an unknown ergodic dynamical system from observations with unknown noise   \n",
       "\n",
       "                               doi  \\\n",
       "256            10.1155/2008/468693   \n",
       "713     10.1016/j.crma.2006.09.025   \n",
       "760     10.1016/j.jpaa.2007.05.019   \n",
       "712   10.1016/j.neucom.2005.12.010   \n",
       "668       10.1088/1009-9271/8/1/13   \n",
       "148              10.1119/1.2919743   \n",
       "385  10.1088/1748-0221/2/11/P11007   \n",
       "109        10.1073/pnas.0700324104   \n",
       "130     10.1103/PhysRevB.76.155307   \n",
       "521              10.1214/07-AOS562   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 abstract  \\\n",
       "256                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Noise, corruptions and variations in face images can seriously hurt the performance of face recognition systems. To make such systems robust, multiclass neuralnetwork classifiers capable of learning from noisy data have been suggested. However on large face data sets such systems cannot provide the robustness at a high level. In this paper we explore a pairwise neural-network system as an alternative approach to improving the robustness of face recognition. In our experiments this approach is shown to outperform the multiclass neural-network system in terms of the predictive accuracy on the face images corrupted by noise.    \n",
       "713                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               This Note proposes a new methodology for function classification with Support Vector Machine (SVM). Rather than relying on projection on a truncated Hilbert basis as in our previous work, we use an implicit spline interpolation that allows us to compute SVM on the derivatives of the studied functions. To that end, we propose a kernel defined directly on the discretizations of the observed functions. We show that this method is universally consistent.    \n",
       "760                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             It is well known that Pythagorean triples can be parametrized by two triples of polynomials with integer coefficients. We show that no single triple of polynomials with integer coefficients in any number of variables is sufficient, but that there exists a parametrization of Pythagorean triples by a single triple of integer-valued polynomials.    \n",
       "712                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     In many applications, input data are sampled functions taking their values in infinite dimensional spaces rather than standard vectors. This fact has complex consequences on data analysis algorithms that motivate modifications of them. In fact most of the traditional data analysis tools for regression, classification and clustering have been adapted to functional inputs under the general name of functional Data Analysis (FDA). In this paper, we investigate the use of Support Vector Machines (SVMs) for functional data analysis and we focus on the problem of curves discrimination. SVMs are large margin classifier tools based on implicit non linear mappings of the considered data into high dimensional spaces thanks to kernels. We show how to define simple kernels that take into account the unctional nature of the data and lead to consistent classification. Experiments conducted on real world data emphasize the benefit of taking into account some functional aspects of the problems.    \n",
       "668                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We investigate two training-set methods: support vector machines (SVMs) and Kernel Regression (KR) for photometric redshift estimation with the data from the Sloan Digital Sky Survey Data Release 5 and Two Micron All Sky Survey databases. We probe the performances of SVMs and KR for different input patterns. Our experiments show that the more parameters considered, the accuracy doesn't always increase, and only when appropriate parameters chosen, the accuracy can improve. Moreover for different approaches, the best input pattern is different. With different parameters as input, the optimal bandwidth is dissimilar for KR. The rms errors of photometric redshifts based on SVM and KR methods are less than 0.03 and 0.02, respectively. Finally the strengths and weaknesses of the two approaches are summarized. Compared to other methods of estimating photometric redshifts, they show their superiorities, especially KR, in terms of accuracy.    \n",
       "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A small tabletop experiment for a direct measurement of the speed of light to an accuracy of few percent is described. The experiment is accessible to a wide spectrum of undergraduate students, in particular to students not majoring in science or engineering. The experiment may further include a measurement of the index of refraction of a sample. Details of the setup and equipment are given. Results and limitations of the experiment are analyzed, partly based on our experience in employing the experiment in our student laboratories. Safety considerations are also discussed.    \n",
       "385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                This paper compares the performance of artificial neural networks and boosted decision trees, with and without cascade training, for tagging b-jets in a collider experiment. It is shown, using a Monte Carlo simulation of WH → lν qq̅ events, that for a b-tagging efficiency of 50   \n",
       "109    Recent experimental studies indicate that synaptic changes induced by neuronal activity are discrete jumps between a small number of stable states. Learning in systems with discrete synapses is known to be a computationally hard problem. Here, we study a neurobiologically plausible on-line learning algorithm that derives from Belief Propagation algorithms. We show that it performs remarkably well in a model neuron with binary synapses, and a finite number of `hidden' states per synapse, that has to learn a random classification task. Such system is able to learn a number of associations close to the theoretical limit, in time which is sublinear in system size. This is to our knowledge the first on-line algorithm that is able to achieve efficiently a finite number of patterns learned per binary synapse. Furthermore, we show that performance is optimal for a finite number of hidden states which becomes very small for sparse coding. The algorithm is similar to the standard `perceptron' learning algorithm, with an additional rule for synaptic transitions which occur only if a currently presented pattern is `barely correct'. In this case, the synaptic changes are meta-plastic only (change in hidden states and not in actual synaptic state), stabilizing the synapse in its current state. Finally, we show that a system with two visible states and K hidden states is much more robust to noise than a system with K visible states. We suggest this rule is sufficiently simple to be easily implemented by neurobiological systems or in hardware.    \n",
       "130                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We explore the full counting statistics of single electron tunneling through a quantum dot using a quantum point contact as non-invasive high bandwidth charge detector. The distribution of counted tunneling events is measured as a function of gate and source-drain-voltage for several consecutive electron numbers on the quantum dot. For certain configurations we observe super-Poissonian statistics for bias voltages at which excited states become accessible. The associated counting distributions interestingly show a bimodal characteristic. Analyzing the time dependence of the number of electron counts we relate this to a slow switching between different electron configurations on the quantum dot.    \n",
       "521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        We consider the problem of forecasting the next (observable) state of an unknown ergodic dynamical system from a noisy observation of the present state. Our main result shows, for example, that support vector machines (SVMs) using Gaussian RBF kernels can learn the best forecaster from a sequence of noisy observations if (a) the unknown observational noise process is bounded and has a summable α-mixing rate and (b) the unknown ergodic dynamical system is defined by a Lipschitz continuous function on some compact subset of ℝ^d and has a summable decay of correlations for Lipschitz continuous functions. In order to prove this result we first establish a general consistency result for SVMs and all stochastic processes that satisfy a mixing notion that is substantially weaker than α-mixing.    \n",
       "\n",
       "    update_date     score  \n",
       "256  2016-02-17  0.700318  \n",
       "713  2007-05-23  0.699330  \n",
       "760  2011-06-29  0.674661  \n",
       "712  2007-05-23  0.668528  \n",
       "668  2009-11-13  0.656348  \n",
       "148  2009-11-13  0.652193  \n",
       "385  2011-01-27  0.649132  \n",
       "109  2009-11-13  0.648897  \n",
       "130  2009-11-13  0.646445  \n",
       "521  2009-04-07  0.642360  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 recommendation based on similarity score\n",
    "data1.sort_values('score', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "literature_survey_recc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
